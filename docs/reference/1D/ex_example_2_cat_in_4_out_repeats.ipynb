{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46f6531",
   "metadata": {},
   "source": [
    "# example_2_cat_in_4_out_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"This file demonstrates benchmarking with categorical inputs and multiple outputs with repeats.\n",
    "\n",
    "It simulates comparing programming languages and development environments, measuring\n",
    "performance and developer productivity metrics.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import bencher as bch\n",
    "\n",
    "random.seed(42)  # Fixed seed for reproducibility\n",
    "\n",
    "\n",
    "class ProgrammingBenchmark(bch.ParametrizedSweep):\n",
    "    \"\"\"Benchmark class comparing programming languages and development environments.\"\"\"\n",
    "\n",
    "    language = bch.StringSweep(\n",
    "        [\"Python\", \"JavaScript\", \"Rust\", \"Go\"], doc=\"Programming language being benchmarked\"\n",
    "    )\n",
    "    environment = bch.StringSweep(\n",
    "        [\"Development\", \"Testing\", \"Production\"], doc=\"Environment configuration\"\n",
    "    )\n",
    "\n",
    "    execution_time = bch.ResultVar(units=\"ms\", doc=\"Execution time in milliseconds\")\n",
    "    memory_usage = bch.ResultVar(units=\"MB\", doc=\"Memory usage in megabytes\")\n",
    "\n",
    "    def __call__(self, **kwargs) -> dict:\n",
    "        \"\"\"Execute the parameter sweep for the given inputs.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional parameters to update before executing\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the outputs of the parameter sweep\n",
    "        \"\"\"\n",
    "        self.update_params_from_kwargs(**kwargs)\n",
    "\n",
    "        # Base values that will be modified by language and environment\n",
    "        base_execution = 0\n",
    "        base_memory = 0\n",
    "\n",
    "        # Different languages have different performance characteristics\n",
    "        if self.language == \"Python\":\n",
    "            base_execution = 150\n",
    "            base_memory = 80\n",
    "        elif self.language == \"JavaScript\":\n",
    "            base_execution = 100\n",
    "            base_memory = 60\n",
    "        elif self.language == \"Rust\":\n",
    "            base_execution = 20\n",
    "            base_memory = 15\n",
    "        elif self.language == \"Go\":\n",
    "            base_execution = 40\n",
    "            base_memory = 30\n",
    "\n",
    "        # Environment affects performance\n",
    "        if self.environment == \"Development\":\n",
    "            # Dev environments have debugging overhead\n",
    "            env_exec_modifier = 1.5\n",
    "            env_mem_modifier = 1.3\n",
    "        elif self.environment == \"Testing\":\n",
    "            # Testing has moderate overhead\n",
    "            env_exec_modifier = 1.2\n",
    "            env_mem_modifier = 1.1\n",
    "        else:  # Production\n",
    "            # Production is optimized\n",
    "            env_exec_modifier = 1.0\n",
    "            env_mem_modifier = 1.0\n",
    "\n",
    "        # Calculate final values with some randomness\n",
    "        self.execution_time = base_execution * env_exec_modifier * random.uniform(0.9, 1.1)\n",
    "        self.memory_usage = base_memory * env_mem_modifier * random.uniform(0.95, 1.05)\n",
    "\n",
    "        return super().__call__(**kwargs)\n",
    "\n",
    "\n",
    "def example_2_cat_in_4_out_repeats(\n",
    "    run_cfg: bch.BenchRunCfg = None, report: bch.BenchReport = None\n",
    ") -> bch.Bench:\n",
    "    \"\"\"This example compares performance metrics across programming languages and environments.\n",
    "\n",
    "    It demonstrates how to sample categorical variables with multiple repeats\n",
    "    and plot the results of two output variables.\n",
    "\n",
    "    Args:\n",
    "        run_cfg: Configuration for the benchmark run\n",
    "        report: Report to append the results to\n",
    "\n",
    "    Returns:\n",
    "        bch.Bench: The benchmark object\n",
    "    \"\"\"\n",
    "\n",
    "    if run_cfg is None:\n",
    "        run_cfg = bch.BenchRunCfg()\n",
    "    run_cfg.repeats = 15  # Run multiple times to get statistical significance\n",
    "    bench = ProgrammingBenchmark().to_bench(run_cfg, report)\n",
    "    bench.plot_sweep(\n",
    "        title=\"Programming Language and Environment Performance Metrics\",\n",
    "        description=\"Comparing execution time and memory usage across different programming languages and environments\",\n",
    "    )\n",
    "    return bench\n",
    "\n",
    "\n",
    "bench = example_2_cat_in_4_out_repeats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "bench.get_result().to_auto_plots()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
