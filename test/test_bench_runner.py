# Generated by CodiumAI
import unittest
from unittest.mock import Mock
from bencher.example.benchmark_data import SimpleBenchClass, SimpleBenchClassFloat
import bencher as bch


class TestBenchRunner(unittest.TestCase):
    # Tests that bch.BenchRunner can be created with default configuration and the import statement in the bch.BenchRunner class is fixed
    def test_benchrunner_default_configuration_fixed(self):
        bench_runner = bch.BenchRunner("bench_runner_test")
        self.assertEqual(bench_runner.run_cfg.cache_samples, True)
        self.assertEqual(bench_runner.run_cfg.only_hash_tag, True)
        self.assertEqual(bench_runner.run_cfg.level, 2)
        self.assertEqual(bench_runner.publisher, None)
        self.assertEqual(bench_runner.bench_fns, [])

    # Tests that Benchable functions can be added to bch.BenchRunner instance
    def test_benchrunner_add_benchable_functions(self):
        bench_runner = bch.BenchRunner("bench_runner_test")
        bench_fn1 = Mock()
        bench_fn2 = Mock()
        bench_runner.add_run(bench_fn1)
        bench_runner.add_run(bench_fn2)
        self.assertEqual(len(bench_runner.bench_fns), 2)
        self.assertIn(bench_fn1, bench_runner.bench_fns)
        self.assertIn(bench_fn2, bench_runner.bench_fns)

    def test_benchrunner_handle_empty_list(self):
        bench_runner = bch.BenchRunner("bench_runner_test")
        results = bench_runner.run()
        self.assertEqual(len(results), 0)

    def test_benchrunner_benchable_class(self):
        bench_runner = bch.BenchRunner("bench_runner_test")
        bench_runner.add_bench(SimpleBenchClass())
        results = bench_runner.run(run_cfg=bch.BenchRunCfg(run_tag="1"))

        self.assertEqual(results[0].bench_cfg.run_tag, "1")

    def test_benchrunner_cache(self):
        from datetime import datetime

        run_tag = str(datetime.now())

        bench_runner = bch.BenchRunner(
            "bench_runner_test_cache", run_cfg=bch.BenchRunCfg(run_tag=run_tag)
        )

        bench_class = SimpleBenchClass()
        # bench = bch.Bench("test_bench", bench_class, run_cfg=run_cfg, report=report            )

        def run_bench_class(run_cfg: bch.BenchRunCfg, report: bch.BenchReport) -> bch.BenchCfg:
            bench = bch.Bench("test_bench1_cache", bench_class, run_cfg=run_cfg, report=report)
            bench.plot_sweep("bench_1")
            return bench

        bench_runner.add_run(run_bench_class)

        # run with unique tag and with cache, should not hit cache because unique tag
        results = bench_runner.run()
        self.assertEqual(results[0].sample_cache.worker_wrapper_call_count, 2)
        self.assertEqual(results[0].sample_cache.worker_fn_call_count, 2)
        self.assertEqual(results[0].sample_cache.worker_cache_call_count, 0)
        self.assertEqual(results[0].run_cfg.run_tag, run_tag)

        # run again with the same tag, should hit cache because it was already run
        results = bench_runner.run()
        self.assertEqual(results[0].sample_cache.worker_wrapper_call_count, 2)
        self.assertEqual(results[0].sample_cache.worker_fn_call_count, 2)
        self.assertEqual(results[0].sample_cache.worker_cache_call_count, 0)
        self.assertEqual(results[0].run_cfg.run_tag, run_tag)

        # run with the same tag but set use cache to false, should not hit cache because even tho the tag is the same, cache_results=false
        results = bench_runner.run(cache_results=False)
        self.assertEqual(results[0].sample_cache.worker_wrapper_call_count, 2)
        self.assertEqual(results[0].sample_cache.worker_fn_call_count, 2)
        self.assertEqual(results[0].sample_cache.worker_cache_call_count, 0)
        self.assertEqual(results[0].run_cfg.run_tag, run_tag)

    def test_benchrunner_benchable_class_run_constructor(self):
        bench_runner = bch.BenchRunner("bench_runner_test", run_cfg=bch.BenchRunCfg(run_tag="1"))
        bench_runner.add_bench(SimpleBenchClass())
        results = bench_runner.run()
        self.assertEqual(results[0].bench_cfg.run_tag, "1")

    # def test_benchrunner_level_1(self):
    #     results = bch.BenchRunner("bench_runner_test", AllSweepVars()).run(min_level=1)
    #     self.assertEqual(results[0].result_samples(), 1)

    # def test_benchrunner_level_1_only(self):
    #     results = bch.BenchRunner("bench_runner_test", AllSweepVars()).run(level=1)
    #     self.assertEqual(results[0].result_samples(), 1)

    def test_benchrunner_repeats(self):
        res = bch.Bench(
            "float", SimpleBenchClassFloat(), run_cfg=bch.BenchRunCfg(level=2, repeats=1)
        ).plot_sweep("float")
        self.assertEqual(res.result_samples(), 2)

        res = bch.Bench(
            "float", SimpleBenchClassFloat(), run_cfg=bch.BenchRunCfg(level=2, repeats=5)
        ).plot_sweep("float")
        self.assertEqual(res.result_samples(), 10)

    def test_benchrunner_unified_interface(self):
        """Test the new unified interface with level/repeats and max_level/max_repeats."""
        # Track what configurations are run
        executed_configs = []

        def simple_benchmark(run_cfg: bch.BenchRunCfg, report: bch.BenchReport) -> bch.BenchCfg:
            executed_configs.append((run_cfg.level, run_cfg.repeats))
            bench = bch.Bench("test", SimpleBenchClassFloat(), run_cfg=run_cfg, report=report)
            return bench.plot_sweep("test")

        # Test 1: Single level and repeats (no max values)
        br1 = bch.BenchRunner()
        br1.add(simple_benchmark)
        executed_configs.clear()
        results = br1.run(level=2, repeats=1)
        self.assertEqual(len(results), 1)
        self.assertEqual(len(executed_configs), 1)
        self.assertEqual(executed_configs[0], (2, 1))

        # Test 2: Progressive levels (level=2, max_level=3, repeats=1)
        br2 = bch.BenchRunner()
        br2.add(simple_benchmark)
        executed_configs.clear()
        results = br2.run(level=2, repeats=1, max_level=3)
        # Should run at level 2 and 3 = 2 results
        levels = sorted([config[0] for config in executed_configs])
        self.assertEqual(levels, [2, 3])
        self.assertEqual(len(executed_configs), 2)

        # Test 3: Progressive repeats (level=2, repeats=1, max_repeats=2)
        br3 = bch.BenchRunner()
        br3.add(simple_benchmark)
        executed_configs.clear()
        results = br3.run(level=2, repeats=1, max_repeats=2)
        # Should run with repeats 1 and 2 = 2 results
        repeats = sorted([config[1] for config in executed_configs])
        self.assertEqual(repeats, [1, 2])
        self.assertEqual(len(executed_configs), 2)

        # Test 4: Both progressive (level=2-3, repeats=1-2) = 4 combinations
        br4 = bch.BenchRunner()
        br4.add(simple_benchmark)
        executed_configs.clear()
        results = br4.run(level=2, repeats=1, max_level=3, max_repeats=2)
        # Check all combinations were executed
        expected_combinations = [(2, 1), (2, 2), (3, 1), (3, 2)]
        self.assertEqual(sorted(executed_configs), sorted(expected_combinations))
        self.assertEqual(len(executed_configs), 4)

    def test_benchrunner_deprecation_warnings(self):
        """Test that legacy parameters show deprecation warnings."""
        import warnings

        bench_runner = bch.BenchRunner("test_deprecation")

        # Very simple function that returns immediately
        def simple_test(run_cfg: bch.BenchRunCfg, report: bch.BenchReport) -> bch.BenchCfg:  # pylint: disable=unused-argument
            cfg = bch.BenchCfg()
            cfg.run_cfg = run_cfg
            return cfg

        bench_runner.add_run(simple_test)

        # Capture warnings manually to avoid import issues
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            bench_runner.run(min_level=3)
            self.assertTrue(
                any("min_level parameter is deprecated" in str(warning.message) for warning in w)
            )

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            bench_runner.run(start_repeats=2)
            self.assertTrue(
                any(
                    "start_repeats parameter is deprecated" in str(warning.message) for warning in w
                )
            )

    def test_benchrunner_no_name_instantiation(self):
        """Test that BenchRunner can be instantiated without a name."""
        bench_runner = bch.BenchRunner()

        # Should have auto-generated name
        self.assertIsNotNone(bench_runner.name)
        self.assertIsInstance(bench_runner.name, str)
        self.assertTrue(bench_runner.name.startswith("bench_runner_"))

        # Should work normally
        def test_benchmark(run_cfg: bch.BenchRunCfg, report: bch.BenchReport) -> bch.BenchCfg:
            bench = bch.Bench("test", SimpleBenchClassFloat(), run_cfg=run_cfg, report=report)
            return bench.plot_sweep("test")

        bench_runner.add(test_benchmark)
        results = bench_runner.run(level=2, repeats=1)
        self.assertEqual(len(results), 1)

    # def test_benchrunner_cache(self):
    #     res = bch.Bench(
    #         "float", SimpleBenchClassFloat(), run_cfg=bch.BenchRunCfg(level=2, repeats=1)
    #     ).plot_sweep("float")

    #     res = bch.Bench(
    #         "float", SimpleBenchClassFloat(), run_cfg=bch.BenchRunCfg(level=2, repeats=5)
    #     ).plot_sweep("float")
    #     self.assertEqual(res.result_samples(), 10)

    # # Tests that bch.BenchRunner can run Benchable functions with default configuration (fixed)
    # def test_benchrunner_run_default_configuration_fixed(self):

    #     bench_runner = bch.BenchRunner()
    #     bench_fn1 = Mock()
    #     bench_fn2 = Mock()
    #     bench_runner.add_run(bench_fn1)
    #     bench_runner.add_run(bench_fn2)
    #     results = bench_runner.run()

    #     self.assertEqual(len(results), 10)
    #     self.assertEqual(results[0].level, 1)
    #     self.assertEqual(results[1].level, 1)
    #     self.assertEqual(results[2].level, 2)
    #     self.assertEqual(results[3].level, 2)
    #     self.assertEqual(results[4].level, 3)
    #     self.assertEqual(results[5].level, 3)
    #     self.assertEqual(results[6].level, 4)
    #     self.assertEqual(results[7].level, 4)
    #     self.assertEqual(results[8].level, 5)
    #     self.assertEqual(results[9].level, 5)

    # Tests that bch.BenchRunner can run Benchable functions with custom configuration, after fixing the import statements
    # def test_benchrunner_run_custom_configuration_fixed_fixed_import_statements(self):

    #     bench_runner = bch.BenchRunner()
    #     bench_fn1 = Mock()
    #     bench_fn2 = Mock()
    #     bench_runner.add_run(bench_fn1)
    #     bench_runner.add_run(bench_fn2)
    #     run_cfg = bch.BenchRunCfg()
    #     run_cfg.cache_samples = False
    #     run_cfg.only_hash_tag = False
    #     run_cfg.level = 3
    #     results = bench_runner.run(run_cfg=run_cfg)
    #     self.assertEqual(len(results), 2)
    #     self.assertEqual(results[0].level, 3)
    #     self.assertEqual(results[1].level, 3)

    # Tests that bch.BenchRunner can publish results of Benchable functions (fixed)
    # def test_benchrunner_publish_results_fixed(self):
    #     class MockBenchable:
    #         def bench(self, run_cfg: bch.BenchRunCfg) -> bch.BenchCfg:
    #             return bch.BenchCfg()

    #     bench_runner = bch.BenchRunner(publisher=Mock())
    #     bench_fn1 = MockBenchable()
    #     bench_fn2 = MockBenchable()
    #     bench_runner.add_run(bench_fn1)
    #     bench_runner.add_run(bench_fn2)
    #     results = bench_runner.run(publish=True)
    #     self.assertEqual(len(results), 10)
    #     self.assertEqual(bench_runner.publisher.call_count, 10)
    #     self.assertEqual(bench_runner.publisher.call_args_list[0][0][0], results[0])
    #     self.assertEqual(bench_runner.publisher.call_args_list[1][0][0], results[1])
    #     self.assertEqual(bench_runner.publisher.call_args_list[2][0][0], results[2])
    #     self.assertEqual(bench_runner.publisher.call_args_list[3][0][0], results[3])
    #     self.assertEqual(bench_runner.publisher.call_args_list[4][0][0], results[4])
    #     self.assertEqual(bench_runner.publisher.call_args_list[5][0][0], results[5])
    #     self.assertEqual(bench_runner.publisher.call_args_list[6][0][0], results[6])
    #     self.assertEqual(bench_runner.publisher.call_args_list[7][0][0], results[7])
    #     self.assertEqual(bench_runner.publisher.call_args_list[8][0][0], results[8])
    #     self.assertEqual(bench_runner.publisher.call_args_list[9][0][0], results[9])

    # Tests that bch.BenchRunner can handle empty list of Benchable functions

    # Tests that bch.BenchRunner can handle empty list of Benchable functions
    # def test_benchrunner_handle_empty_list(self):

    #     def benchable(run_cfg:bch.BenchRunCfg)->bch.BenchCfg:
    #         bench = bch.Bench("sbc",SimpleBenchClass(),run_cfg=run_cfg)
    #         return bench.plot_sweep("sweep1")

    #     bench_runner = bch.BenchRunner()
    #     bench_runner.add_run(benchable)

    #     results = bench_runner.run(run_cfg=bch.BenchRunCfg(run_tag="1"))

    #     self.assertEqual(results[0].bench_cfg.run_tag, "1")

    # Tests that bch.BenchRunner can handle empty list of Benchable functions
